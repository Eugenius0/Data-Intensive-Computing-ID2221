{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ee7fe5-9155-4b84-bb09-26eed5ef1ca8",
   "metadata": {},
   "source": [
    "# Healthcare Data Analysis for Predictive Modeling\n",
    "\n",
    "The project aims to predict the likelihood of diseases, such as diabetes or heart disease, using patient health records. By analyzing historical healthcare data, the goal is to build predictive models that can identify individuals at high risk for these conditions. This can assist healthcare providers in early detection and personalized treatment plans, potentially improving patient outcomes and reducing healthcare costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795dfe1f-d5a3-435b-bb12-86175393b001",
   "metadata": {},
   "source": [
    "# Dataset Overview and Preprocessing\n",
    "\n",
    "The dataset includes patient information such as age, sex, chest pain type, blood pressure, cholesterol levels, and exercise-induced angina. These features are used to predict the presence or absence of heart disease. Key preprocessing steps include handling missing values, type casting, and encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da38882-7dae-4ad0-9bef-f2c1225d9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the DynamoDB client (local instance)\n",
    "dynamodb = boto3.resource(\n",
    "    'dynamodb',\n",
    "    endpoint_url='http://dynamodb-local:8000',  # Local DynamoDB URL\n",
    "    region_name='us-west-2',               # AWS region (optional for local)\n",
    "    aws_access_key_id='dummy',             # Dummy credentials for local use\n",
    "    aws_secret_access_key='dummy'          # Dummy credentials for local use\n",
    ")\n",
    "\n",
    "DB_TABLE_NAME = \"processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed77efba-9df4-4bc9-949d-4d1adaf4b548",
   "metadata": {},
   "source": [
    "# Creating a DynamoDB Table for Data Storage\n",
    "\n",
    "Here, we set up a DynamoDB table to store the processed data for each patient. Each entry is given a unique identifier and stored with attributes corresponding to patient details and heart disease indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11eafad-309d-479e-8c39-2005038fb8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'processed' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create DB Table\n",
    "# Define table parameters\n",
    "table_name = DB_TABLE_NAME\n",
    "attribute_definitions = [\n",
    "    {\n",
    "        'AttributeName': 'pk',\n",
    "        'AttributeType': 'S'  # S for String, N for Number, B for Binary\n",
    "    }\n",
    "]\n",
    "key_schema = [\n",
    "    {\n",
    "        'AttributeName': 'pk',\n",
    "        'KeyType': 'HASH'  # Partition key\n",
    "    }\n",
    "]\n",
    "provisioned_throughput = {\n",
    "    'ReadCapacityUnits': 1,\n",
    "    'WriteCapacityUnits': 1\n",
    "}\n",
    "\n",
    "# Create the table\n",
    "try:\n",
    "    table = dynamodb.create_table(\n",
    "        TableName=table_name,\n",
    "        AttributeDefinitions=attribute_definitions,\n",
    "        KeySchema=key_schema,\n",
    "        ProvisionedThroughput=provisioned_throughput\n",
    "    )\n",
    "    # Wait until the table is created\n",
    "    table.meta.client.get_waiter('table_exists').wait(TableName=table_name)\n",
    "    print(f\"Table '{table_name}' created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e1d90-aed4-47b7-b863-2005bfcb062e",
   "metadata": {},
   "source": [
    "### Reading the Healthcare Dataset\n",
    "We load and inspect the dataset, which contains various patient attributes such as age, sex, blood pressure, cholesterol levels, and more. This dataset is crucial for building a predictive model for heart disease detection.\n",
    "\n",
    "The dataset is stored on HDFS and after loading, we assign meaningful column names to facilitate further analysis and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968d4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|63.0|1.0|1.0|   145.0|233.0|1.0|    2.0|  150.0|  0.0|    2.3|  3.0|0.0| 6.0|     0|\n",
      "|67.0|1.0|4.0|   160.0|286.0|0.0|    2.0|  108.0|  1.0|    1.5|  2.0|3.0| 3.0|     2|\n",
      "|67.0|1.0|4.0|   120.0|229.0|0.0|    2.0|  129.0|  1.0|    2.6|  2.0|2.0| 7.0|     1|\n",
      "|37.0|1.0|3.0|   130.0|250.0|0.0|    0.0|  187.0|  0.0|    3.5|  3.0|0.0| 3.0|     0|\n",
      "|41.0|0.0|2.0|   130.0|204.0|0.0|    2.0|  172.0|  0.0|    1.4|  1.0|0.0| 3.0|     0|\n",
      "|56.0|1.0|2.0|   120.0|236.0|0.0|    0.0|  178.0|  0.0|    0.8|  1.0|0.0| 3.0|     0|\n",
      "|62.0|0.0|4.0|   140.0|268.0|0.0|    2.0|  160.0|  0.0|    3.6|  3.0|2.0| 3.0|     3|\n",
      "|57.0|0.0|4.0|   120.0|354.0|0.0|    0.0|  163.0|  1.0|    0.6|  1.0|0.0| 3.0|     0|\n",
      "|63.0|1.0|4.0|   130.0|254.0|0.0|    2.0|  147.0|  0.0|    1.4|  2.0|1.0| 7.0|     2|\n",
      "|53.0|1.0|4.0|   140.0|203.0|1.0|    2.0|  155.0|  1.0|    3.1|  3.0|0.0| 7.0|     1|\n",
      "|57.0|1.0|4.0|   140.0|192.0|0.0|    0.0|  148.0|  0.0|    0.4|  2.0|0.0| 6.0|     0|\n",
      "|56.0|0.0|2.0|   140.0|294.0|0.0|    2.0|  153.0|  0.0|    1.3|  2.0|0.0| 3.0|     0|\n",
      "|56.0|1.0|3.0|   130.0|256.0|1.0|    2.0|  142.0|  1.0|    0.6|  2.0|1.0| 6.0|     2|\n",
      "|44.0|1.0|2.0|   120.0|263.0|0.0|    0.0|  173.0|  0.0|    0.0|  1.0|0.0| 7.0|     0|\n",
      "|52.0|1.0|3.0|   172.0|199.0|1.0|    0.0|  162.0|  0.0|    0.5|  1.0|0.0| 7.0|     0|\n",
      "|57.0|1.0|3.0|   150.0|168.0|0.0|    0.0|  174.0|  0.0|    1.6|  1.0|0.0| 3.0|     0|\n",
      "|48.0|1.0|2.0|   110.0|229.0|0.0|    0.0|  168.0|  0.0|    1.0|  3.0|0.0| 7.0|     1|\n",
      "|54.0|1.0|4.0|   140.0|239.0|0.0|    0.0|  160.0|  0.0|    1.2|  1.0|0.0| 3.0|     0|\n",
      "|48.0|0.0|3.0|   130.0|275.0|0.0|    0.0|  139.0|  0.0|    0.2|  1.0|0.0| 3.0|     0|\n",
      "|49.0|1.0|2.0|   130.0|266.0|0.0|    0.0|  171.0|  0.0|    0.6|  1.0|0.0| 3.0|     0|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean, col, when, count, isnan\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HealthcareDataAnalysis\").getOrCreate()\n",
    "\n",
    "columns = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "]\n",
    "\n",
    "data = spark.read.csv(\"hdfs://namenode:9000/input/heart_decease.csv\", header=False, inferSchema=True)\n",
    "\n",
    "data = data.toDF(*columns)\n",
    "\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9585e-862c-4380-b62f-4bd9945f6a47",
   "metadata": {},
   "source": [
    "## Column Headers and Their Descriptions\n",
    "\n",
    "**age**: Age of the patient in years.\n",
    "\n",
    "**sex**: Sex of the patient.\n",
    "- `1` = Male\n",
    "- `0` = Female\n",
    "\n",
    "**cp**: Chest pain type (4 types).\n",
    "- `1` = Typical angina: chest pain related to decreased blood supply to the heart.\n",
    "- `2` = Atypical angina: chest pain not related to the heart.\n",
    "- `3` = Non-anginal pain: typically esophageal or another form of chest pain.\n",
    "- `4` = Asymptomatic: no chest pain.\n",
    "\n",
    "**trestbps**: Resting blood pressure (in mm Hg on admission to the hospital).\n",
    "- This is the blood pressure measured while at rest.\n",
    "\n",
    "**chol**: Serum cholesterol in mg/dl.\n",
    "- Represents the cholesterol level of the patient.\n",
    "\n",
    "**fbs**: Fasting blood sugar > 120 mg/dl.\n",
    "- `1` = True (fasting blood sugar is higher than 120 mg/dl)\n",
    "- `0` = False (fasting blood sugar is lower than or equal to 120 mg/dl)\n",
    "\n",
    "**restecg**: Resting electrocardiographic results (values 0, 1, 2).\n",
    "- `0` = Normal.\n",
    "- `1` = Having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV).\n",
    "- `2` = Showing probable or definite left ventricular hypertrophy by Estes' criteria.\n",
    "\n",
    "**thalach**: Maximum heart rate achieved.\n",
    "- The highest heart rate reached during a stress test.\n",
    "\n",
    "**exang**: Exercise-induced angina.\n",
    "- `1` = Yes (angina induced by exercise)\n",
    "- `0` = No (no angina induced by exercise)\n",
    "\n",
    "**oldpeak**: ST depression induced by exercise relative to rest.\n",
    "- Represents the difference between the heart's state during rest and during exercise.\n",
    "\n",
    "**slope**: The slope of the peak exercise ST segment.\n",
    "- `1` = Upsloping: better heart rate recovery.\n",
    "- `2` = Flat: minimal or no change in the ST segment.\n",
    "- `3` = Downsloping: indicative of heart problems.\n",
    "\n",
    "**ca**: Number of major vessels (0-3) colored by fluoroscopy.\n",
    "- Represents the number of major blood vessels that are visible after injecting a contrast dye.\n",
    "- The values range from `0` to `3`.\n",
    "\n",
    "**thal**: Thalassemia (a blood disorder that affects hemoglobin levels).\n",
    "- `3` = Normal.\n",
    "- `6` = Fixed defect: no proper blood movement in part of the heart.\n",
    "- `7` = Reversible defect: blood flow is impaired, but not permanently.\n",
    "\n",
    "**target**: Diagnosis of heart disease (angiographic disease status).\n",
    "- `0` = No presence of heart disease.\n",
    "- `1` = Presence of heart disease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2feb1f-c5e0-4134-abb1-370cb0d94972",
   "metadata": {},
   "source": [
    "### Verifying DynamoDB Table Connectivity\n",
    "To confirm that our DynamoDB table is set up correctly, we insert a sample entry with a unique identifier (`pk`) and a sample attribute (`Name`). The success of this operation indicates that the DynamoDB table is ready for storing our processed healthcare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cf3f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'fa214045-baae-43e9-af79-5c8f0f7e1679',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Jetty(12.0.8)',\n",
       "   'date': 'Sat, 26 Oct 2024 14:55:07 GMT',\n",
       "   'x-amzn-requestid': 'fa214045-baae-43e9-af79-5c8f0f7e1679',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'x-amz-crc32': '2745614147',\n",
       "   'content-length': '2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persist data\n",
    "table = dynamodb.Table(DB_TABLE_NAME)\n",
    "\n",
    "# Put an item into the table\n",
    "table.put_item(\n",
    "    Item={\n",
    "        'pk': '123',\n",
    "        'Name': 'Example Item'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b0cb93-e27b-42fc-ba63-2f77c482a689",
   "metadata": {},
   "source": [
    "### Storing Processed Data in DynamoDB\n",
    "To store patient data, we iterate through each record in our DataFrame, converting numeric fields to the `Decimal` format compatible with DynamoDB. Each record is uniquely identified using a combination of the patient's `age` and `sex` attributes, ensuring traceable and organized storage in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30589a3d-69f5-446e-938e-cb563e8b45d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in DynamoDB successfully.\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal, ROUND_HALF_UP, InvalidOperation\n",
    "\n",
    "# Helper function to safely convert to Decimal\n",
    "def safe_decimal(value):\n",
    "    # Check if the value is None or non-numeric\n",
    "    if value is None:\n",
    "        return None\n",
    "    try:\n",
    "        return Decimal(value).quantize(Decimal(\"0.01\"), rounding=ROUND_HALF_UP)\n",
    "    except (InvalidOperation, TypeError):\n",
    "        return None  # Return None or another placeholder if conversion fails\n",
    "\n",
    "# Iterate through each row in the DataFrame and store it in DynamoDB\n",
    "for row in data.collect():\n",
    "    table.put_item(\n",
    "        Item={\n",
    "            'pk': f\"patient_{int(row['age'])}_{int(row['sex'])}\",  # Unique ID combining age and sex\n",
    "            'age': safe_decimal(row['age']),\n",
    "            'sex': 'Male' if row['sex'] == 1 else 'Female',\n",
    "            'cp': safe_decimal(row['cp']),\n",
    "            'trestbps': safe_decimal(row['trestbps']),\n",
    "            'chol': safe_decimal(row['chol']),\n",
    "            'fbs': safe_decimal(row['fbs']),\n",
    "            'restecg': safe_decimal(row['restecg']),\n",
    "            'thalach': safe_decimal(row['thalach']),\n",
    "            'exang': safe_decimal(row['exang']),\n",
    "            'oldpeak': safe_decimal(row['oldpeak']),\n",
    "            'slope': safe_decimal(row['slope']),\n",
    "            'ca': safe_decimal(row['ca']),\n",
    "            'thal': safe_decimal(row['thal']),\n",
    "            'target': safe_decimal(row['target'])\n",
    "        }\n",
    "    )\n",
    "print(\"Data stored in DynamoDB successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77ca40-c541-454e-a93d-f786599465a2",
   "metadata": {},
   "source": [
    "### Retrieving Sample Data from DynamoDB\n",
    "To verify successful data storage, we retrieve a few sample records from the DynamoDB table. This step ensures that the data structure aligns with expectations and that each attribute is stored as intended. This retrieval can be useful for validation before proceeding to any further data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "541ed702-b546-44bc-82ab-553263f13907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved items from DynamoDB:\n",
      "{'exang': Decimal('1'), 'sex': 'Male', 'thal': Decimal('6'), 'chol': Decimal('169'), 'slope': Decimal('3'), 'cp': Decimal('4'), 'trestbps': Decimal('120'), 'target': Decimal('2'), 'oldpeak': Decimal('2.8'), 'thalach': Decimal('144'), 'fbs': Decimal('0'), 'pk': 'patient_44_1', 'age': Decimal('44'), 'ca': Decimal('0'), 'restecg': Decimal('0')}\n",
      "{'exang': Decimal('1'), 'sex': 'Female', 'thal': Decimal('3'), 'chol': Decimal('243'), 'slope': Decimal('2'), 'cp': Decimal('4'), 'trestbps': Decimal('138'), 'target': Decimal('0'), 'oldpeak': Decimal('0'), 'thalach': Decimal('152'), 'fbs': Decimal('0'), 'pk': 'patient_46_0', 'age': Decimal('46'), 'ca': Decimal('0'), 'restecg': Decimal('2')}\n",
      "{'exang': Decimal('0'), 'sex': 'Female', 'thal': Decimal('3'), 'chol': Decimal('306'), 'slope': Decimal('1'), 'cp': Decimal('2'), 'trestbps': Decimal('126'), 'target': Decimal('0'), 'oldpeak': Decimal('0'), 'thalach': Decimal('163'), 'fbs': Decimal('0'), 'pk': 'patient_41_0', 'age': Decimal('41'), 'ca': Decimal('0'), 'restecg': Decimal('0')}\n",
      "{'exang': Decimal('0'), 'sex': 'Male', 'thal': Decimal('3'), 'chol': Decimal('149'), 'slope': Decimal('1'), 'cp': Decimal('3'), 'trestbps': Decimal('118'), 'target': Decimal('1'), 'oldpeak': Decimal('0.8'), 'thalach': Decimal('126'), 'fbs': Decimal('0'), 'pk': 'patient_49_1', 'age': Decimal('49'), 'ca': Decimal('3'), 'restecg': Decimal('2')}\n",
      "{'exang': Decimal('1'), 'sex': 'Male', 'thal': Decimal('7'), 'chol': Decimal('131'), 'slope': Decimal('2'), 'cp': Decimal('4'), 'trestbps': Decimal('130'), 'target': Decimal('3'), 'oldpeak': Decimal('1.2'), 'thalach': Decimal('115'), 'fbs': Decimal('0'), 'pk': 'patient_57_1', 'age': Decimal('57'), 'ca': Decimal('1'), 'restecg': Decimal('0')}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key\n",
    "\n",
    "# Initialize the DynamoDB client (assuming local instance)\n",
    "dynamodb = boto3.resource(\n",
    "    'dynamodb',\n",
    "    endpoint_url='http://dynamodb-local:8000',  # Local DynamoDB URL\n",
    "    region_name='us-west-2',\n",
    "    aws_access_key_id='dummy',\n",
    "    aws_secret_access_key='dummy'\n",
    ")\n",
    "\n",
    "# Specify the table name\n",
    "table = dynamodb.Table('processed')\n",
    "\n",
    "# Retrieve a few items from DynamoDB\n",
    "try:\n",
    "    # Scan the table to retrieve all items (useful for validation)\n",
    "    response = table.scan(Limit=5)  # Retrieve 5 items to check for data accuracy\n",
    "    \n",
    "    # Print the retrieved items\n",
    "    items = response.get('Items', [])\n",
    "    print(\"Retrieved items from DynamoDB:\")\n",
    "    for item in items:\n",
    "        print(item)\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving items: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd97c791-7cf7-4892-a5b0-d2725472b6b2",
   "metadata": {},
   "source": [
    "### Loading Dataset from HDFS into Spark DataFrame\n",
    "This code loads the heart disease dataset stored in HDFS into a Spark DataFrame, allowing efficient handling of large data volumes. Columns are renamed to clarify their meanings, aligning with the dataset schema and a sample of the data is displayed to confirm successful loading and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c245c7af-5412-418d-aee8-5bd641c72053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|63.0|1.0|1.0|   145.0|233.0|1.0|    2.0|  150.0|  0.0|    2.3|  3.0|0.0| 6.0|     0|\n",
      "|67.0|1.0|4.0|   160.0|286.0|0.0|    2.0|  108.0|  1.0|    1.5|  2.0|3.0| 3.0|     2|\n",
      "|67.0|1.0|4.0|   120.0|229.0|0.0|    2.0|  129.0|  1.0|    2.6|  2.0|2.0| 7.0|     1|\n",
      "|37.0|1.0|3.0|   130.0|250.0|0.0|    0.0|  187.0|  0.0|    3.5|  3.0|0.0| 3.0|     0|\n",
      "|41.0|0.0|2.0|   130.0|204.0|0.0|    2.0|  172.0|  0.0|    1.4|  1.0|0.0| 3.0|     0|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data from HDFS\n",
    "data = spark.read.csv(\"hdfs://namenode:9000/input/heart_decease.csv\", header=False, inferSchema=True)\n",
    "\n",
    "# Rename columns as needed\n",
    "columns = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "]\n",
    "data = data.toDF(*columns)\n",
    "\n",
    "# Display to confirm successful loading\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990b8bf-8568-4bd4-88ec-f5e663f42888",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Feature Engineering\n",
    "This section preprocesses the data to make it suitable for machine learning models. Key steps include:\n",
    "- Converting necessary columns to `float` type for compatibility.\n",
    "- Handling missing values by dropping rows with `NA` values.\n",
    "- Encoding categorical variables (e.g., `sex`) to numerical format.\n",
    "- Assembling feature columns into a single vector (`features`) needed for modeling.\n",
    "\n",
    "The preprocessed data now has a `features` column with a vector of values and a `target` column for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1132d9b-41a9-48f9-ba2e-9239c34d5bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 15:25:40 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|target|\n",
      "+--------------------+------+\n",
      "|[63.0,1.0,145.0,2...|     0|\n",
      "|[67.0,4.0,160.0,2...|     2|\n",
      "|[67.0,4.0,120.0,2...|     1|\n",
      "|[37.0,3.0,130.0,2...|     0|\n",
      "|[41.0,2.0,130.0,2...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "# Convert columns to float type to ensure compatibility with VectorAssembler\n",
    "data = data.withColumn(\"ca\", col(\"ca\").cast(\"float\"))\n",
    "data = data.withColumn(\"thal\", col(\"thal\").cast(\"float\"))\n",
    "\n",
    "# Check and remove the 'sex_indexed' column if it already exists\n",
    "if 'sex_indexed' in data.columns:\n",
    "    data = data.drop('sex_indexed')\n",
    "\n",
    "# Handle missing values (if any exist, drop for simplicity here)\n",
    "data = data.na.drop()\n",
    "\n",
    "# Index categorical columns like 'sex'\n",
    "indexer = StringIndexer(inputCol=\"sex\", outputCol=\"sex_indexed\")\n",
    "data = indexer.fit(data).transform(data)\n",
    "\n",
    "# Assemble features into a single vector for training\n",
    "feature_columns = [col for col in data.columns if col not in ['target', 'sex']]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data).select(\"features\", \"target\")\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4262d-9660-45bb-a437-590d4c9f0674",
   "metadata": {},
   "source": [
    "### Baseline Model Training: Logistic Regression\n",
    "This section establishes a baseline model using Logistic Regression to predict heart disease likelihood based on patient features. The process includes:\n",
    "- **Data Splitting**: Dividing data into training (70%) and test (30%) sets.\n",
    "- **Model Training**: Training a Logistic Regression model on the training data.\n",
    "- **Predictions**: Generating predictions on the test data.\n",
    "- **Evaluation**: Calculating model accuracy to understand its initial performance.\n",
    "\n",
    "Initial results show a model accuracy of 66%, providing a starting point for further model optimization and comparison with other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f8e118a-eb7f-45da-93c9-496933cbdad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 15:27:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/10/26 15:27:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.66\n",
      "+--------------------+------+----------+\n",
      "|            features|target|prediction|\n",
      "+--------------------+------+----------+\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     1|       0.0|\n",
      "+--------------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Step 1: Split the data into training and testing sets\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Step 2: Initialize and train the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"target\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Step 3: Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Show some predictions for inspection\n",
    "predictions.select(\"features\", \"target\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbab643-7470-44c3-a999-56bf4a9bbdb6",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Standardizing feature values is crucial for machine learning models that depend on the scale of input data, especially when combining features of different units and magnitudes. Here, we:\n",
    "- **Scale Features**: Using `StandardScaler`, all features are scaled to have a mean of 0 and standard deviation of 1.\n",
    "- **Output Adjustments**: The transformed `scaled_features` column is renamed back to `features` for seamless model integration.\n",
    "\n",
    "With standardized features, models can make more consistent predictions, unaffected by variations in individual feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b67d72ff-dec1-41b0-ab36-8f0652bdc869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|target|\n",
      "+--------------------+------+\n",
      "|[6.96152928881618...|     0|\n",
      "|[7.40353114842356...|     2|\n",
      "|[7.40353114842356...|     1|\n",
      "|[4.08851720136823...|     0|\n",
      "|[4.53051906097561...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Make sure 'features' column only exists once\n",
    "data = data.select(\"features\", \"target\")\n",
    "\n",
    "# Apply StandardScaler on the 'features' column\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(data)\n",
    "scaled_data = scaler_model.transform(data)\n",
    "\n",
    "# Select the scaled features and target, renaming 'scaled_features' to 'features' for the model input\n",
    "final_data = scaled_data.select(col(\"scaled_features\").alias(\"features\"), \"target\")\n",
    "\n",
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ce299-0a95-4a80-91e7-c2ee2e9f1c7e",
   "metadata": {},
   "source": [
    "### Checking Class Distribution\n",
    "To understand the class balance in our dataset, we count the instances of each target class:\n",
    "- **Class Balance**: Imbalance in classes can bias the model, causing it to favor the majority class.\n",
    "- **Insights**: Here, the `target=0` class has the most samples, indicating an imbalance. We'll consider balancing techniques in subsequent steps to address this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64d6a176-2eae-4433-8eb0-16707c465132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     1|   54|\n",
      "|     3|   35|\n",
      "|     4|   13|\n",
      "|     2|   35|\n",
      "|     0|  160|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "final_data.groupBy(\"target\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec24ebb-5e79-4336-8e3d-cda456fb18c6",
   "metadata": {},
   "source": [
    "### Oversampling Minority Classes\n",
    "To handle class imbalance, we oversample the minority classes in the dataset:\n",
    "- **Purpose**: Balancing the dataset ensures the model learns features from each class equally.\n",
    "- **Method**: We use a custom function to duplicate instances in the minority classes (`target=1, 2, 3, 4`) based on a specified sample ratio.\n",
    "- **Result**: After oversampling, each class has an improved representation, enhancing the model's prediction capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2118c239-d23e-43c9-a1a4-83b81ae59196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     0|  160|\n",
      "|     1|  118|\n",
      "|     2|   80|\n",
      "|     3|   69|\n",
      "|     4|   18|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Function to oversample the minority classes\n",
    "def oversample(df: DataFrame, target_col: str, sample_ratio: float) -> DataFrame:\n",
    "    # Separate the majority and minority classes\n",
    "    majority_class = df.filter(f\"{target_col} = 0\")\n",
    "    other_classes = df.filter(f\"{target_col} != 0\")\n",
    "    \n",
    "    # Oversample each minority class and union them\n",
    "    oversampled_df = majority_class\n",
    "    for label in [1, 2, 3, 4]:\n",
    "        sampled_class = other_classes.filter(f\"{target_col} = {label}\").sample(withReplacement=True, fraction=sample_ratio)\n",
    "        oversampled_df = oversampled_df.union(sampled_class)\n",
    "        \n",
    "    return oversampled_df\n",
    "\n",
    "# Oversample the minority classes (tune sample_ratio as needed)\n",
    "balanced_data = oversample(final_data, \"target\", sample_ratio=2.0)\n",
    "balanced_data.groupBy(\"target\").count().show()  # Check the new class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37de2e8-efd9-45e1-a453-061a3ea02a1d",
   "metadata": {},
   "source": [
    "### Training and Evaluating Logistic Regression on Balanced Data\n",
    "To improve predictive performance, we use a Logistic Regression model trained on the balanced data:\n",
    "- **Data Split**: We split the data into 70% training and 30% test sets.\n",
    "- **Model Training**: Logistic Regression is chosen for its simplicity and interpretability.\n",
    "- **Evaluation Metrics**:\n",
    "  - **Accuracy**: Measures the overall correctness of predictions.\n",
    "  - **F1 Score**: Accounts for both precision and recall, especially useful for imbalanced datasets.\n",
    "- **Result**: This step allows us to assess the Logistic Regression model's performance on the balanced data before moving to more complex models.\n",
    "\n",
    "### Logistic Regression Performance on Balanced Data\n",
    "Upon evaluating the Logistic Regression model on balanced data, the results show:\n",
    "- **Model Accuracy**: 0.59\n",
    "- **F1 Score**: 0.59\n",
    "\n",
    "These metrics suggest that the Logistic Regression model struggles with classifying the target variable accurately. Given this, exploring more complex algorithms, such as Random Forest or Decision Trees, could improve predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fda29d2a-b577-424c-8138-f3c795671654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the balanced data into training and test sets\n",
    "train_data, test_data = balanced_data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bce568c2-9dbd-44af-bec4-df29f08b0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"target\", maxIter=10)\n",
    "model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec718800-afd0-4748-a723-56f52d2376da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.59\n",
      "F1 Score: 0.59\n",
      "+--------------------+------+----------+\n",
      "|            features|target|prediction|\n",
      "+--------------------+------+----------+\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "+--------------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n",
    "\n",
    "# Show a few sample predictions\n",
    "predictions.select(\"features\", \"target\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2cd836-3ec9-4b50-aa84-6017f788c588",
   "metadata": {},
   "source": [
    "### Random Forest Model Performance\n",
    "Using a Random Forest classifier with 100 trees, the model achieved:\n",
    "- **Model Accuracy**: 0.76\n",
    "- **F1 Score**: 0.76\n",
    "\n",
    "This improvement over Logistic Regression suggests that Random Forest is more effective in capturing the data's complexity. The sample predictions demonstrate a better distribution of predictions across classes, although some misclassification remains. Further tuning of the Random Forest parameters or exploring other algorithms may enhance these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a0708b0-07ab-4c80-b00e-49323f7b25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestClassifier(featuresCol=\"features\", labelCol=\"target\", numTrees=100)\n",
    "\n",
    "# Train the model\n",
    "tree_model = model.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = tree_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "048cf304-f26f-40f3-86b4-04bfdf82feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.76\n",
      "F1 Score: 0.76\n",
      "+--------------------+------+----------+\n",
      "|            features|target|prediction|\n",
      "+--------------------+------+----------+\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       1.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "|(13,[0,1,2,3,6,9,...|     0|       0.0|\n",
      "+--------------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-run evaluation on the test data\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n",
    "\n",
    "# Show a few sample predictions to understand class distribution\n",
    "predictions.select(\"features\", \"target\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880386d8-e172-4b12-81f6-06a272707f7d",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with Cross-Validation\n",
    "To optimize the Random Forest model, cross-validation with a parameter grid was applied. This tuning tested multiple configurations:\n",
    "- **numTrees**: Number of trees (values: 50, 100, 150)\n",
    "- **maxDepth**: Maximum depth of trees (values: 5, 10, 15)\n",
    "\n",
    "After cross-validation, the best model achieved:\n",
    "- **Tuned Random Forest Accuracy**: 0.81\n",
    "- **Tuned Random Forest F1 Score**: 0.81\n",
    "\n",
    "These results demonstrate that tuning hyperparameters significantly improved model performance, suggesting that the tuned model better generalizes to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55c81bcd-7546-40e1-9313-6970fd107c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Random Forest Accuracy: 0.81\n",
      "Tuned Random Forest F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Configure Spark session to limit broadcast size\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)  # Disables broadcasting for joins\n",
    "\n",
    "# Reduce logging level to suppress DAGScheduler warnings if they persist (optional)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Set to \"ERROR\" to only see critical warnings\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define evaluator and parameter grid for tuning\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf_model.numTrees, [50, 100, 150]) \\\n",
    "    .addGrid(rf_model.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Setup CrossValidator for tuning\n",
    "crossval = CrossValidator(estimator=rf_model, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Apply best model from cross-validation to test data\n",
    "rf_tuned_predictions = cv_model.bestModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_tuned_accuracy = evaluator.evaluate(rf_tuned_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "rf_tuned_f1 = evaluator.evaluate(rf_tuned_predictions)\n",
    "\n",
    "print(f\"Tuned Random Forest Accuracy: {rf_tuned_accuracy:.2f}\")\n",
    "print(f\"Tuned Random Forest F1 Score: {rf_tuned_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc720c55-3cf3-4903-a1b8-11352c38aa36",
   "metadata": {},
   "source": [
    "### Feature Importances\n",
    "The numbers correspond to the contribution of each feature (higher values mean the feature has a larger impact on predictions):\n",
    "\n",
    "- **age**: 0.1367\n",
    "- **sex**: 0.0627\n",
    "- **cp (chest pain type)**: 0.1009\n",
    "- **trestbps (resting blood pressure)**: 0.0963\n",
    "- **chol (cholesterol)**: 0.0196\n",
    "- **fbs (fasting blood sugar)**: 0.0251\n",
    "- **restecg (resting ECG results)**: 0.1445\n",
    "- **thalach (maximum heart rate achieved)**: 0.0423\n",
    "- **exang (exercise-induced angina)**: 0.1377\n",
    "- **oldpeak (ST depression induced by exercise)**: 0.0372\n",
    "- **slope (slope of the peak exercise ST segment)**: 0.0812\n",
    "- **ca (number of major vessels colored by fluoroscopy)**: 0.0731\n",
    "- **thal (thalassemia type)**: 0.0427\n",
    "\n",
    "### Key Insights\n",
    "- **Top Features**: `restecg`, `age`, `exang`, and `cp` are among the top contributors, suggesting that they are significant indicators of heart disease likelihood in this dataset.\n",
    "- **Low Impact Features**: `chol` and `fbs` have relatively low feature importance, indicating they have a smaller effect on predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "282379ac-16fd-44b5-8cde-3c414142a116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: (13,[0,1,2,3,4,5,6,7,8,9,10,11,12],[0.13671807808413206,0.06273086359763451,0.10095429121553508,0.0962744782030749,0.019580941444494866,0.02510959181143669,0.14448491866333338,0.04226907342420461,0.137696543492306,0.037196738904549304,0.08117716395702822,0.0731145382315963,0.0426927789706739])\n"
     ]
    }
   ],
   "source": [
    "feature_importances = cv_model.bestModel.featureImportances\n",
    "print(\"Feature Importances:\", feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12ed6f-35c8-485a-a14f-19f9954e7530",
   "metadata": {},
   "source": [
    "### Storing Results in DynamoDB\n",
    "To facilitate future retrieval and analysis, the predictions and feature importances are stored in a DynamoDB table:\n",
    "\n",
    "1. **Predictions**: Each prediction entry includes the input features, actual target, and model-predicted target.\n",
    "2. **Feature Importances**: Feature importance scores are saved in a structured format, allowing insights into which features contributed most to the model's predictions.\n",
    "\n",
    "Upon running this code, a success message confirms the data storage process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b41690e-2f77-4b06-a3db-f3faaa641938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions and feature importances saved to DynamoDB successfully!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from decimal import Decimal\n",
    "\n",
    "# Initialize the DynamoDB client\n",
    "dynamodb = boto3.resource(\n",
    "    'dynamodb',\n",
    "    endpoint_url='http://dynamodb-local:8000',  # Adjust if needed\n",
    "    region_name='us-west-2',\n",
    "    aws_access_key_id='dummy',\n",
    "    aws_secret_access_key='dummy'\n",
    ")\n",
    "\n",
    "# Define or create the table\n",
    "table_name = 'ModelResults'\n",
    "try:\n",
    "    table = dynamodb.Table(table_name)\n",
    "    table.load()  # Verify if table exists\n",
    "except Exception:\n",
    "    # Create table if it doesn't exist\n",
    "    table = dynamodb.create_table(\n",
    "        TableName=table_name,\n",
    "        KeySchema=[{'AttributeName': 'id', 'KeyType': 'HASH'}],\n",
    "        AttributeDefinitions=[{'AttributeName': 'id', 'AttributeType': 'S'}],\n",
    "        ProvisionedThroughput={'ReadCapacityUnits': 1, 'WriteCapacityUnits': 1}\n",
    "    )\n",
    "    table.meta.client.get_waiter('table_exists').wait(TableName=table_name)\n",
    "\n",
    "# Save predictions\n",
    "for i, row in enumerate(rf_tuned_predictions.collect()):\n",
    "    table.put_item(\n",
    "        Item={\n",
    "            'id': f\"sample_{i}\",\n",
    "            'features': [Decimal(str(x)) for x in row['features']],\n",
    "            'actual_target': int(row['target']),\n",
    "            'predicted_target': int(row['prediction'])\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Save feature importances\n",
    "table.put_item(\n",
    "    Item={\n",
    "        'id': 'feature_importances',\n",
    "        'importances': {col: Decimal(str(imp)) for col, imp in zip(feature_columns, feature_importances)}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Predictions and feature importances saved to DynamoDB successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d8804-deb3-43cf-bdee-628db6acc2d3",
   "metadata": {},
   "source": [
    "## Conclusion and Summary\n",
    "\n",
    "This project applied machine learning techniques to healthcare data with the aim of predicting the likelihood of heart disease. Through careful data preprocessing, oversampling of minority classes, and model optimization, a tuned Random Forest model achieved an accuracy and F1 score of 0.81. Feature importance analysis highlighted key indicators, such as resting ECG results and exercise-induced angina, as critical predictors of heart disease.\n",
    "\n",
    "The final predictions and feature importances were stored in DynamoDB, enabling scalable, persistent access for further analysis and integration with other applications. This project illustrates how machine learning models can support early diagnosis and preventive healthcare, potentially leading to more personalized and effective patient care strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0df258-96dc-4920-b3b1-dcaa465de651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
